# -*- coding: utf-8 -*-
"""
Created on Fri Mar  3 15:18:43 2017

@author: ZIQIAOLIU
"""
"""
###########################
#####Radndom Forest########
###########################
"""

import numpy as np
import sklearn.cross_validation as cv
import sklearn.grid_search as gs
from sklearn import ensemble
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
import sklearn.metrics


x_train, x_test, y_train, y_test = cv.train_test_split(x, y, 
                                                       test_size=0.2, random_state=123)


randomForest = ensemble.RandomForestClassifier()
np.random.seed(123)

"""
Parameter tuning for Random Forest
"""
"""
Method 1: test by setting number of tree is 50
"""
randomForest.set_params(n_estimators=50)
randomForest.fit(x_train, y_train)

print "The training error of random forest is: %.5f" %(1-randomForest.score(x_train, y_train))
print "The test     error of random forest is: %.5f" %(1-randomForest.score(x_test, y_test))
"""
The training error of random forest is: 0.00000
The test     error of random forest is: 0.13542
"""
"""
Model Performance Evulation
"""
predictions_rf1=randomForest.predict(x_test)
print sklearn.metrics.confusion_matrix(y_test,predictions_rf1)
"""
[[29  0  0]
 [ 1 32  8]
 [ 0  4 22]]
"""
print sklearn.metrics.accuracy_score(y_test, predictions_rf1)   
"""    
0.864583333333
"""
"""
test with feature plan 1
"""
x_train_f1, x_test_f1, y_train_f1, y_test_f1 = cv.train_test_split(f1_x, y, 
                                                       test_size=0.2, random_state=123)

randomForest_f1 = ensemble.RandomForestClassifier()
randomForest_f1.set_params(n_estimators=50)
randomForest_f1.fit(x_train_f1, y_train_f1)
predictions_rf1_f1=randomForest_f1.predict(x_test_f1)
print sklearn.metrics.confusion_matrix(y_test_f1,predictions_rf1_f1)
print sklearn.metrics.accuracy_score(y_test_f1, predictions_rf1_f1)  

"""
[[29  0  0]
 [ 2 30  9]
 [ 0  1 25]]
0.875     
The accuracy improves
 
"""

"""
test with feature plan 2
"""
x_train_f2, x_test_f2, y_train_f2, y_test_f2 = cv.train_test_split(f2_x, y, 
                                                       test_size=0.2, random_state=123)

randomForest_f2 = ensemble.RandomForestClassifier()
randomForest_f2.set_params(n_estimators=50)
randomForest_f2.fit(x_train_f2, y_train_f2)
predictions_rf2_f2=randomForest_f2.predict(x_test_f2)
print sklearn.metrics.confusion_matrix(y_test_f2,predictions_rf2_f2)
print sklearn.metrics.accuracy_score(y_test_f2, predictions_rf2_f2)  

"""
[[28  1  0]
 [ 2 35  4]
 [ 0  6 20]]
0.864583333333    
The accuracy score does not change, which could be utilized in improving modeling
performance based on small dimension data generated by feature plan 2.
 
"""

"""
Method 2: using cross validation, gridsearch but will take 
some time to finish search
"""

grid_para_forest = [{"n_estimators": [10, 50, 100], "criterion": ["gini", "entropy"], \
                    "min_samples_leaf": range(1, 10), "min_samples_split": np.linspace(2, 30, 15)}]
grid_search_forest = gs.GridSearchCV(randomForest, grid_para_forest, scoring='accuracy', cv=5)

grid_search_forest.fit(x_train, y_train)

grid_search_forest.best_params_
"""
###results##
#{'criterion': 'gini',
# 'min_samples_leaf': 1,
# 'min_samples_split': 2.0,
# 'n_estimators': 100}
"""
grid_search_forest.best_score_
"""
0.7734375
"""

print "The training error is: %.5f"%(1-grid_search_forest.score(x_train, y_train))
print "The test     error is: %.5f"%(1-grid_search_forest.score(x_test, y_test)
"""
The training error is: 0.00000
The test     error is: 0.16667
"""
"""
Method 3: select best number of trees
"""
tree_number = range(10, 500, 10)
train_error1 = []
test_error1 = []
oob_error1 = []

np.random.seed(1)
for i in tree_number:
    randomForest.set_params(n_estimators = i, oob_score=True)
    randomForest.fit(x_train, y_train)
    train_error1.append(1 - randomForest.score(x_train, y_train))
    test_error1.append(1 - randomForest.score(x_test, y_test))
    oob_error1.append(1 - randomForest.oob_score_)
    
import matplotlib.pyplot as plt
plt.plot(tree_number, train_error1, c = 'red', label = 'training error')
plt.plot(tree_number, test_error1, c = 'blue', label = 'test error')
plt.plot(tree_number, oob_error1, c = 'pink', label = 'oob error')
plt.xlabel('Number of Trees')
plt.title('Process of selecting the number of trees')
plt.legend()
plt.show()
"""
the best number of tree values is around 100
"""
"""
Method 4: select the best depth of trees
"""
tree_depth = range(3,20,1)
train_error2 = []
test_error2 = []
oob_error2 = []

np.random.seed(1)
for i in tree_depth:
    randomForest.set_params(max_depth=i, oob_score=True)
    randomForest.fit(x_train, y_train)
    train_error2.append(1 - randomForest.score(x_train, y_train))
    test_error2.append(1 - randomForest.score(x_test, y_test))
    oob_error2.append(1 - randomForest.oob_score_)
    
import matplotlib.pyplot as pl
plt.plot(tree_depth, train_error2, c = 'red', label = 'training error')
plt.plot(tree_depth, test_error2, c = 'blue', label = 'test error')
plt.plot(tree_depth, oob_error2, c = 'pink', label = 'oob error')
plt.xlabel('Depth of Trees')
plt.title('Process of selecting the depth of trees')
plt.legend()
plt.show()

"""
the best number is around 11
"""




